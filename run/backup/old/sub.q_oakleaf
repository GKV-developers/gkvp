#!/bin/csh

###  NOTE  ###
###    Process number x Thread number =< 16 Cores x Node numbers
###    Available resource groups:
###      debug           #   1- 240 nodes, 30 min., 28GB/nodes
###      short           #   1-  72 nodes,  6 hour, 28GB/nodes
###      regular         #  12- 480 nodes, 48 hour, 28GB/nodes
###      x-large         # 481-1440 nodes, 24 hour, 28GB/nodes
###      challenge       #   1-4800 nodes, 24 hour, 28GB/nodes
###      interactive_n1  #        1 nodes,  2 hour, 28GB/nodes, interactive
###      interactive_n8  #        8 nodes, 10 min., 28GB/nodes, interactive
###
###    To submit a interactive job, "pjsub --interact sub.q"
###                    a batch job, "pjsub sub.q"
###                      step jobs, "pjsub --step sub1.q sub2.q ..."
##############

#PJM -L "rscgrp=debug"
#PJM -L "node=64"
#PJM -L "elapse=00:15:00"
#PJM -j
#PJM --mpi "proc=128"
#### --mpi "rank-map-hostfile=myrankmap"
#PJM -g "gc15"

setenv PARALLEL 8          # Thread number for automatic parallelization
setenv OMP_NUM_THREADS 8   # Thread number for Open MP


set DIR=%%DIR%%
set LDM=gkvp_mpifft.exe
set NL=gkvp_namelist.%%%

### Run
cd ${DIR}
setenv fu05 ${DIR}/${NL}

date
#rm -rf ${DIR}/Fprofd*
#mkdir ${DIR}/Fprofd_Stati
#fapp -C -d ${DIR}/Fprofd_Stati -Impi,hwm -L1 -Hevent=Statistics mpiexec ${DIR}/${LDM}
#fipp -C -d ${DIR}/Fprofd_Stati -Ihwm -Srange mpiexec ${DIR}/${LDM}
mpiexec ${DIR}/${LDM}
date 
touch complete

