#!/bin/bash

###  NOTE  ###
###  Plasma Simulator Subsystem A, NEC LX 204Bin-3 (NIFS-QST, 2025)
###
###  - Computation nodes (Total 360 nodes)
###      CPU: Intel Xeon 6980P (2.0GHz, 43(42)cores*3NUMA=128cores) x2 per node
###      Peak performance: DP 16.384 TFLOPS per node
###      Memory: 768 GB (32GB MCR-8800 MRDIMM ECC x24)
###      Memory Bandwidth: 1689.8 GB/s per node
###
###      Therefore, a recommended GKV parallelization may be 
###          (MPI Processes)x(OpenMP Threads)
###          =< (42 cores per NUMA)x(3 NUMA)x(2 CPU)x(Node numbers)
###      1 or more MPI process should be assigined to 1 NUMA.
###
###  - Queue class (July 2025)
###      A_dev_i :  1 - 32   ncpus,   3 hour,  4 run/4  submit, interactive
###      A_dev   :  1 - 1024 ncpus, 0.5 hour,  4 run/8  submit
###      A_S     :  1 - 1024 ncpus,  24 hour, 10 run/20 submit
###      A_M     :  5 - 64   nodes,  24 hour, 10 run/20 submit
###      A_L     : 65 - 320  nodes,  24 hour,  2 run/4  submit
###
###  - Commands
###      (Submit a batch job : "qsub sub.q")  ->  Use shoot script for GKV.
###      Check job status    : "qstat"
###      Delete job          : "qdel JOBID"
###      Show budget info    : "uresource PROJECTNAME"
###      Show disk usage     : "check_quota"
###
###  - PBS settings
###      -P: Project name
###      -q: Queue class (A_dev, A_S, A_M, A_L)
###      -l walltime: Execute time
###      -l select: Computation resource request
###      *** Example of resource request ***
###      Ex. 1)
###          -l select=36:ncpus=256:mem=752gb:mpiprocs=24
###                 select=36 requests 36 resource chunks.
###                 Each chunk specifies ncpus=256 cores and mem=752 GB, 
###                 corresponding to 1 compute node. 
###                 mpiprocs=24 means 24 MPI processes per chunk.
###                 -> Total: 36 computation nodes and 24x36=864 MPI processes
###      Ex. 2)
###          -l select=72:ncpus=128:mem=376gb:mpiprocs=12
###                 An equivalent configuration with the above, 
###                 while each chunk corresponds to 1 CPU socket.
##############

#PBS -P NIFS23KIST041
#PBS -q A_dev
#PBS -l walltime=00:30:00
#PBS -l select=1:ncpus=128:mem=376gb:mpiprocs=16

export OMP_NUM_THREADS=8   # Set number of OpenMP threads per MPI process



### Settings
ulimit -s unlimited
source /system/apps/rhel9/dev/oneapi/2025.1/setvars.sh # Load Intel oneAPI
#export FI_PROVIDER=mlx            # (Commented) Use Mellanox/UCX communication layer
#export UCX_NET_DEVICES=mlx5_0:1   # (Commented) Specify Infiniband NIC and port for UCX
export FI_PROVIDER=psm3            # Use Intel PSM3 for low-latency communication (Omni-Path)
#export I_MPI_DEBUG=5              # (Commented) Enable detailed Intel MPI debug output
export I_MPI_DEBUG=0               # Disable Intel MPI debug output (for product run)
export I_MPI_PIN_DOMAIN=omp        # Pin MPI processes based on OpenMP thread domains
#export I_MPI_PIN_ORDER="scatter"  # (Commented) Distribute MPI processes across sockets to reduce contention
export I_MPI_PIN_ORDER="bunch"     # Place MPI processes close together to improve locality


### Log
cd ${PBS_O_WORKDIR}
JOBID=${PBS_JOBID%%.*}
JOBID=${JOBID#*:}
numactl -H > numa.log_${JOBID}
lscpu > lscpu.log_${JOBID}
cpuinfo > cpuinfo.log_${JOBID}

NODEFILE="$PBS_NODEFILE"
NUM_NODES=$(sort "$NODEFILE" | uniq | wc -l)
NUM_PROCS=$(wc -l < "$NODEFILE")
PPN=$(( NUM_PROCS / NUM_NODES ))
echo "                  Nodes: ${NUM_NODES}"
echo "    Total MPI processes: ${NUM_PROCS}"
echo " MPI processes per node: ${PPN}"
echo " OpenMP threads per MPI: ${OMP_NUM_THREADS}"


### Working directory 
DIR=%%DIR%%
LDM=gkvp.exe
NL=gkvp_namelist.%%%


#### Run
date
cd ${DIR}
export fu05=${DIR}/${NL}
mpirun -np ${NUM_PROCS} -ppn ${PPN} ./${LDM}
   # -np        "Total number of MPI processes"
   # -ppn       "MPI processes per node"
date

